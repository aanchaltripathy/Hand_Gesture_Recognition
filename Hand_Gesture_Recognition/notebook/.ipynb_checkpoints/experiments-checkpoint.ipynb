{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f515534",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# üñêÔ∏è Hand Gesture Recognition - Experiments\\n\",\n",
    "    \"Data exploration and model testing notebook\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Imports\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"import mediapipe as mp\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1Ô∏è‚É£ Load gesture names\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"with open('../gesture.names', 'r') as f:\\n\",\n",
    "    \"    classNames = f.read().strip().split('\\\\n')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Gestures:\\\", classNames)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2Ô∏è‚É£ Load dataset\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"with open('../data.pickle', 'rb') as f:\\n\",\n",
    "    \"    data = pickle.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"X = data['X']  # landmarks\\n\",\n",
    "    \"y = data['y']  # labels\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Number of samples:\\\", len(X))\\n\",\n",
    "    \"print(\\\"Shape of one sample:\\\", X[0].shape)\\n\",\n",
    "    \"print(\\\"Labels:\\\", set(y))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3Ô∏è‚É£ Visualize some hand landmarks\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"mpHands = mp.solutions.hands\\n\",\n",
    "    \"mpDraw = mp.solutions.drawing_utils\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig = plt.figure(figsize=(5,5))\\n\",\n",
    "    \"for i in range(3):\\n\",\n",
    "    \"    landmarks = np.array(X[i])\\n\",\n",
    "    \"    plt.scatter(landmarks[:,0], landmarks[:,1], label=classNames[y[i]])\\n\",\n",
    "    \"    plt.title(f\\\"Gesture: {classNames[y[i]]}\\\")\\n\",\n",
    "    \"    plt.gca().invert_yaxis()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4Ô∏è‚É£ Load model (SavedModel)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"model = tf.saved_model.load('../mp_hand_gesture')\\n\",\n",
    "    \"infer = model.signatures['serving_default']\\n\",\n",
    "    \"print(\\\"Model loaded successfully\\\")\\n\",\n",
    "    \"print(\\\"Model output keys:\\\", list(infer.structured_outputs.keys()))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5Ô∏è‚É£ Test predictions on sample data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"sample = np.array([X[0]]).astype(np.float32).reshape(1, -1)  # flatten if needed\\n\",\n",
    "    \"output_key = list(infer.structured_outputs.keys())[0]\\n\",\n",
    "    \"prediction = infer(tf.convert_to_tensor(sample))[output_key].numpy()\\n\",\n",
    "    \"pred_class = classNames[np.argmax(prediction)]\\n\",\n",
    "    \"print(\\\"True class:\\\", classNames[y[0]])\\n\",\n",
    "    \"print(\\\"Predicted class:\\\", pred_class)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6Ô∏è‚É£ Quick stats / distribution of gestures\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"\\n\",\n",
    "    \"df = pd.DataFrame({'gesture': [classNames[i] for i in y]})\\n\",\n",
    "    \"sns.countplot(data=df, x='gesture')\\n\",\n",
    "    \"plt.title('Gesture Distribution')\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ‚úÖ Next steps\\n\",\n",
    "    \"- Try predictions on multiple samples\\n\",\n",
    "    \"- Visualize landmarks overlayed on webcam frames\\n\",\n",
    "    \"- Check model performance per gesture\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.9\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
